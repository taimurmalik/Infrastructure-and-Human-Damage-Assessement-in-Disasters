{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46cc6239-dbfe-4832-91b8-92f474277454",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_class\n",
      "Non-Damage    3494\n",
      "Damage         523\n",
      "Name: count, dtype: int64\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 59ms/step - accuracy: 0.4093 - loss: 1.3131 - val_accuracy: 0.2193 - val_loss: 0.8112\n",
      "Epoch 2/10\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 52ms/step - accuracy: 0.4103 - loss: 1.2762 - val_accuracy: 0.4121 - val_loss: 0.7138\n",
      "Epoch 3/10\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 51ms/step - accuracy: 0.3788 - loss: 1.2706 - val_accuracy: 0.2551 - val_loss: 0.7840\n",
      "Epoch 4/10\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 51ms/step - accuracy: 0.2980 - loss: 1.2729 - val_accuracy: 0.4448 - val_loss: 0.7228\n",
      "Epoch 5/10\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 51ms/step - accuracy: 0.5011 - loss: 1.2534 - val_accuracy: 0.3219 - val_loss: 0.7560\n",
      "Epoch 6/10\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 51ms/step - accuracy: 0.3671 - loss: 1.2312 - val_accuracy: 0.2473 - val_loss: 0.7778\n",
      "Epoch 7/10\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 53ms/step - accuracy: 0.4140 - loss: 1.2634 - val_accuracy: 0.3515 - val_loss: 0.7617\n",
      "Epoch 8/10\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 51ms/step - accuracy: 0.4428 - loss: 1.2755 - val_accuracy: 0.2333 - val_loss: 0.7910\n",
      "Epoch 9/10\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 52ms/step - accuracy: 0.3202 - loss: 1.2302 - val_accuracy: 0.4417 - val_loss: 0.7083\n",
      "Epoch 10/10\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 50ms/step - accuracy: 0.4485 - loss: 1.2555 - val_accuracy: 0.3670 - val_loss: 0.7525\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
      "Accuracy: 0.3482587064676617\n",
      "Precision: 0.13134851138353765\n",
      "Recall: 0.7281553398058253\n",
      "F1 Score: 0.22255192878338279\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import pandas as pd  # To load the dataset\n",
    "import os\n",
    "\n",
    "# Verify and load dataset\n",
    "file_path = r\"C:\\Users\\dare2\\OneDrive\\Desktop\\Pre-processed files\\Hurricane_Irma.csv\"  # Replace with the correct file path\n",
    "if not os.path.exists(file_path):\n",
    "    raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "\n",
    "data = pd.read_csv(file_path)  # Adjust based on the file format (e.g., .csv, .txt)\n",
    "\n",
    "# Check for missing or invalid data in 'processed_data'\n",
    "data['processed_data'] = data['processed_data'].fillna('')  # Replace NaNs with empty string\n",
    "data['processed_data'] = data['processed_data'].astype(str)  # Ensure all data is string\n",
    "\n",
    "# Check for class distribution\n",
    "print(data['binary_class'].value_counts())\n",
    "\n",
    "# Convert labels to binary: 'Non-Damage' -> 0, 'Damage' -> 1\n",
    "data['binary_class'] = data['binary_class'].map({'Non-Damage': 0, 'Damage': 1})\n",
    "\n",
    "# Tokenize sentences\n",
    "sentences = [sentence.split() for sentence in data['processed_data']]  # Tokenize sentences\n",
    "labels = data['binary_class'].values  # Extract labels\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(sentences, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Word2Vec model using CBOW technique\n",
    "word2vec_model = Word2Vec(sentences, vector_size=128, window=5, min_count=1, sg=0)  # CBOW (sg=0)\n",
    "\n",
    "# Create embedding matrix\n",
    "vocab_size = len(word2vec_model.wv.index_to_key)\n",
    "embedding_dim = word2vec_model.vector_size\n",
    "embedding_matrix = np.zeros((vocab_size + 1, embedding_dim))\n",
    "\n",
    "word_index = {word: idx + 1 for idx, word in enumerate(word2vec_model.wv.index_to_key)}\n",
    "for word, idx in word_index.items():\n",
    "    embedding_matrix[idx] = word2vec_model.wv[word]\n",
    "\n",
    "# Map words in X_train and X_test to their indices\n",
    "X_train = [[word_index.get(word, 0) for word in sentence] for sentence in X_train]\n",
    "X_test = [[word_index.get(word, 0) for word in sentence] for sentence in X_test]\n",
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen=100)\n",
    "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=100)\n",
    "\n",
    "# Calculate class weights to handle class imbalance\n",
    "class_weights = {0: 1., 1: len(y_train) / sum(y_train == 1)}\n",
    "\n",
    "# Model architecture with regularization\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size + 1, output_dim=embedding_dim, weights=[embedding_matrix], input_length=X_train.shape[1], trainable=False),\n",
    "    Conv1D(128, 5, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.5),  # Added dropout for regularization\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),  # Added dropout for regularization\n",
    "    Dense(1, activation='sigmoid')  # Binary classification\n",
    "])\n",
    "\n",
    "# Compile model with adjusted learning rate\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model with class weights to address imbalance\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, class_weight=class_weights)\n",
    "\n",
    "# Evaluate model\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Print evaluation metrics with zero_division handling\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred, zero_division=1))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred, zero_division=1))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred, zero_division=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7eee0206-f4da-4c3d-884c-606b029c2d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset columns: Index(['tweet_text', 'text_human', 'Multiclass', 'Binary_Class',\n",
      "       'processed_data'],\n",
      "      dtype='object')\n",
      "                                          tweet_text  \\\n",
      "0  RT @MSN: Island of Barbuda 'literally under wa...   \n",
      "1  RT @Reuters: Hurricane Irma threatens luxury T...   \n",
      "2  RT @TheAnonJournal: BREAKING NEWS: Hurricane I...   \n",
      "3  JUST IN: 11PM #Hurricane #Irma update. @ABC7Ne...   \n",
      "4  RT @cnnbrk: Hurricane Irma destroys \"upwards o...   \n",
      "\n",
      "                          text_human      Multiclass Binary_Class  \\\n",
      "0  infrastructure_and_utility_damage  Infrastructure       Damage   \n",
      "1         other_relevant_information      Non-Damage   Non-Damage   \n",
      "2         other_relevant_information      Non-Damage   Non-Damage   \n",
      "3         other_relevant_information      Non-Damage   Non-Damage   \n",
      "4  infrastructure_and_utility_damage  Infrastructure       Damage   \n",
      "\n",
      "                                      processed_data  \n",
      "0      island barbara literally water hurricane irma  \n",
      "1   hurricane irma threatens luxury trump properties  \n",
      "2  breaking news hurricane irma big enough cover ...  \n",
      "3                                          pm update  \n",
      "4  hurricane irma destroys upwards barbara offici...  \n",
      "Binary_Class\n",
      "0    3494\n",
      "1     523\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 118ms/step - accuracy: 0.4763 - loss: 10.8451 - val_accuracy: 0.8600 - val_loss: 8.2168\n",
      "Epoch 2/30\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 98ms/step - accuracy: 0.5231 - loss: 8.9857 - val_accuracy: 0.7652 - val_loss: 7.0242\n",
      "Epoch 3/30\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 92ms/step - accuracy: 0.4099 - loss: 7.4577 - val_accuracy: 0.8600 - val_loss: 5.9660\n",
      "Epoch 4/30\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 96ms/step - accuracy: 0.5559 - loss: 6.4209 - val_accuracy: 0.8600 - val_loss: 5.1092\n",
      "Epoch 5/30\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 93ms/step - accuracy: 0.4129 - loss: 5.6363 - val_accuracy: 0.8600 - val_loss: 4.3428\n",
      "Epoch 6/30\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 98ms/step - accuracy: 0.4409 - loss: 4.9213 - val_accuracy: 0.8600 - val_loss: 3.7083\n",
      "Epoch 7/30\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 97ms/step - accuracy: 0.3791 - loss: 4.3923 - val_accuracy: 0.8600 - val_loss: 3.2380\n",
      "Epoch 8/30\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 96ms/step - accuracy: 0.3755 - loss: 3.9894 - val_accuracy: 0.1400 - val_loss: 3.2387\n",
      "Epoch 9/30\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 95ms/step - accuracy: 0.3965 - loss: 3.6129 - val_accuracy: 0.6205 - val_loss: 2.7346\n",
      "Epoch 10/30\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 96ms/step - accuracy: 0.3919 - loss: 3.3021 - val_accuracy: 0.7138 - val_loss: 2.3902\n",
      "Epoch 11/30\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 95ms/step - accuracy: 0.4446 - loss: 3.0040 - val_accuracy: 0.7745 - val_loss: 2.1956\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step\n",
      "Accuracy: 0.8718905472636815\n",
      "Precision: 1.0\n",
      "Recall: 0.0\n",
      "F1 Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Verify and load dataset\n",
    "file_path = r\"C:\\Users\\dare2\\OneDrive\\Desktop\\Pre-processed files\\Hurricane_Irma.csv\"\n",
    "if not os.path.exists(file_path):\n",
    "    raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Preprocess data\n",
    "data['processed_data'] = data['processed_data'].fillna('').astype(str)\n",
    "\n",
    "# Check column names and structure\n",
    "print(\"Dataset columns:\", data.columns)\n",
    "print(data.head())\n",
    "\n",
    "# Ensure the column 'Binary_Class' exists\n",
    "if 'Binary_Class' not in data.columns:\n",
    "    raise KeyError(\"The column 'Binary_Class' does not exist in the dataset. Verify the dataset structure.\")\n",
    "\n",
    "# Map labels to binary values and check class distribution\n",
    "data['Binary_Class'] = data['Binary_Class'].map({'Non-Damage': 0, 'Damage': 1})\n",
    "print(data['Binary_Class'].value_counts())\n",
    "\n",
    "# Tokenize sentences\n",
    "sentences = [sentence.split() for sentence in data['processed_data']]\n",
    "labels = data['Binary_Class'].values\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(sentences, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences, vector_size=300, window=5, min_count=1, sg=0)\n",
    "\n",
    "# Create embedding matrix\n",
    "vocab_size = len(word2vec_model.wv.index_to_key)\n",
    "embedding_dim = word2vec_model.vector_size\n",
    "embedding_matrix = np.zeros((vocab_size + 1, embedding_dim))\n",
    "\n",
    "word_index = {word: idx + 1 for idx, word in enumerate(word2vec_model.wv.index_to_key)}\n",
    "for word, idx in word_index.items():\n",
    "    embedding_matrix[idx] = word2vec_model.wv[word]\n",
    "\n",
    "X_train = [[word_index.get(word, 0) for word in sentence] for sentence in X_train]\n",
    "X_test = [[word_index.get(word, 0) for word in sentence] for sentence in X_test]\n",
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen=100)\n",
    "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=100)\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = {0: 1., 1: len(y_train) / sum(y_train == 1)}\n",
    "\n",
    "# Build optimized CNN model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size + 1, output_dim=embedding_dim, weights=[embedding_matrix], input_length=X_train.shape[1], trainable=False),\n",
    "    Conv1D(filters=128, kernel_size=3, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "    Conv1D(filters=128, kernel_size=4, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "    Conv1D(filters=256, kernel_size=5, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    BatchNormalization(),\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train, epochs=30, batch_size=64, validation_split=0.2, class_weight=class_weights, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate model\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred, zero_division=1))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred, zero_division=1))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred, zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "901c8546-f4af-47cc-8293-5a29fc851950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 106ms/step - accuracy: 0.5637 - auc: 0.5159 - loss: 6.4812 - val_accuracy: 0.0018 - val_auc: 0.0000e+00 - val_loss: 3.1336\n",
      "Epoch 2/30\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 96ms/step - accuracy: 0.6068 - auc: 0.5799 - loss: 2.5691 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00 - val_loss: 2.0773\n",
      "Epoch 3/30\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 95ms/step - accuracy: 0.6031 - auc: 0.5713 - loss: 1.5361 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00 - val_loss: 1.3988\n",
      "Epoch 4/30\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 96ms/step - accuracy: 0.6075 - auc: 0.6021 - loss: 1.1382 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00 - val_loss: 1.2596\n",
      "Epoch 5/30\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 96ms/step - accuracy: 0.6234 - auc: 0.6048 - loss: 0.9939 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00 - val_loss: 1.1781\n",
      "Epoch 6/30\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 96ms/step - accuracy: 0.6204 - auc: 0.5967 - loss: 0.9081 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00 - val_loss: 1.0710\n",
      "Epoch 7/30\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 97ms/step - accuracy: 0.6108 - auc: 0.5680 - loss: 0.8691 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00 - val_loss: 1.1204\n",
      "Epoch 8/30\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 98ms/step - accuracy: 0.6282 - auc: 0.5875 - loss: 0.8198 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00 - val_loss: 1.0703\n",
      "Epoch 9/30\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 100ms/step - accuracy: 0.6299 - auc: 0.5817 - loss: 0.7958 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00 - val_loss: 1.1438\n",
      "Epoch 10/30\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 98ms/step - accuracy: 0.6206 - auc: 0.5760 - loss: 0.7615 - val_accuracy: 0.0000e+00 - val_auc: 0.0000e+00 - val_loss: 0.9882\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step\n",
      "Accuracy: 0.8706467661691543\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1 Score: 0.0\n",
      "AUC-ROC: 0.550281844244699\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Oversample the minority class\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Build and compile the model (architecture unchanged)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])\n",
    "\n",
    "# Train the model with early stopping\n",
    "model.fit(X_train_res, y_train_res, epochs=30, batch_size=64, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate with AUC and threshold tuning\n",
    "y_pred_proba = model.predict(X_test).flatten()\n",
    "optimal_threshold = 0.5  # Adjust based on AUC analysis if necessary\n",
    "y_pred = (y_pred_proba > optimal_threshold).astype(int)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred, zero_division=1))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred, zero_division=1))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred, zero_division=1))\n",
    "print(\"AUC-ROC:\", roc_auc_score(y_test, y_pred_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52785603-a9fb-4a8b-b81c-143bb0c699a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
